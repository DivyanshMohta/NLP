{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe9AKDPVLWLh",
        "outputId": "1c8a9160-f788-4e18-9328-ddf213ac53ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9YQ_qZk4Goo",
        "outputId": "d74abd5a-eaad-4664-b0fa-a78ec99a132b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accepted for Analyst\n",
            "Accepted for Analyst\n",
            "Accepted for Threat\n",
            "Accepted for Threat\n",
            "Accepted for Vulnerability\n",
            "Accepted for Threat\n"
          ]
        }
      ],
      "source": [
        "# # Tokenization using NLTK\n",
        "# from nltk import word_tokenize, sent_tokenize\n",
        "# sent = \"GeeksforGeeks is a great learning platform.\\\n",
        "# It is one of the best for Computer Science students.\"\n",
        "# print(word_tokenize(sent))\n",
        "# print(sent_tokenize(sent))\n",
        "\n",
        "\n",
        "# from nltk.stem import PorterStemmer\n",
        "\n",
        "# # # create an object of class PorterStemmer\n",
        "# porter = PorterStemmer()\n",
        "\n",
        "# # print(porter.stem(\"play\"))\n",
        "# # print(porter.stem(\"playing\"))\n",
        "# # print(porter.stem(\"plays\"))\n",
        "# # print(porter.stem(\"played\"))\n",
        "\n",
        "\n",
        "# text = \"\"\"Stemming can be particularly useful when you want to\n",
        "#  perform operations like counting word frequencies or analyzing\n",
        "#  document similarity without distinguishing between variations\n",
        "#  of the same word.\n",
        "#  \"\"\"\n",
        "\n",
        "# words = text.split()\n",
        "\n",
        "# stemmed_words = [porter.stem(word) for word in words]\n",
        "\n",
        "# print(\"Original words:\", words)\n",
        "# print(\"Stemmed words:\", stemmed_words)\n",
        "\n",
        "\n",
        "\n",
        "# from nltk import word_tokenize, sent_tokenize\n",
        "# sent = \"Specialization in Network Security. Developed advanced threat detection technologies, reducing the risk of a successful cyber-attack by 15%.\\\n",
        "# With a 2 years of exp in OWASP\"\n",
        "\n",
        "# requriments= ['network', 'owasp', 'cybersecurity', 'threat', 'detection']\n",
        "\n",
        "# resume = word_tokenize(sent)\n",
        "# for skill in resume:\n",
        "#     # print(skill)\n",
        "#     if skill.lower() in requriments:\n",
        "#         print(\"Accepted for\", skill)\n",
        "\n",
        "\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Define the required skills\n",
        "requirements = ['vulnerability', 'analyst',  'siem', 'cybersecurity', 'threat', 'detection', ' cryptography', 'intrusion', 'scanning']\n",
        "\n",
        "# Function to process the resume text file\n",
        "def check_skills_in_resume(file_path):\n",
        "    # Open and read the resume file\n",
        "    with open(file_path, 'r') as file:\n",
        "        resume_text = file.read()\n",
        "\n",
        "    # Tokenize the content of the resume\n",
        "    resume_tokens = word_tokenize(resume_text)\n",
        "\n",
        "    # Check if any token matches the required skills\n",
        "    for skill in resume_tokens:\n",
        "        if skill.lower() in requirements:\n",
        "            print(\"Accepted for\", skill)\n",
        "\n",
        "# Example usage with the file path\n",
        "file_path = 'resume.txt'  # Replace with your file path\n",
        "check_skills_in_resume(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNbrALBj4eIT"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}