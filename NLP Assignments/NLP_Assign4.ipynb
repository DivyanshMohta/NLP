{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpTm64i60E0B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Multi-Head Attention Layer\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_size = embed_size\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_size, \"Embedding size must be divisible by number of heads\"\n",
        "\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.num_heads different pieces\n",
        "        values = values.reshape(N, value_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
        "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
        "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        energy = torch.einsum(\"nhqd,nhkd->nhqk\", [queries, keys])  # (num_heads, N, query_len, key_len)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=-1)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nhld->nhqd\", [attention, values])  # (num_heads, N, query_len, head_dim)\n",
        "        out = out.permute(1, 2, 0, 3).reshape(N, query_len, self.num_heads * self.head_dim)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# 2. Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "# 3. Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, embed_size * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size * 4, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "# 4. Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.encoder_attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.norm3 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, embed_size * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size * 4, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, enc_out, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        query = self.dropout(self.norm1(attention + query))\n",
        "        encoder_attention = self.encoder_attention(enc_out, enc_out, query, mask)\n",
        "        x = self.dropout(self.norm2(encoder_attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm3(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "# 5. Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, num_heads, num_enc_layers, num_dec_layers, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.tgt_word_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(embed_size, num_heads, dropout) for _ in range(num_enc_layers)]\n",
        "        )\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(embed_size, num_heads, dropout) for _ in range(num_dec_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        src_embed = self.positional_encoding(self.src_word_embedding(src))\n",
        "        tgt_embed = self.positional_encoding(self.tgt_word_embedding(tgt))\n",
        "\n",
        "        enc_out = src_embed\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_out = layer(enc_out, enc_out, enc_out, src_mask)\n",
        "\n",
        "        dec_out = tgt_embed\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_out = layer(enc_out, enc_out, dec_out, enc_out, tgt_mask)\n",
        "\n",
        "        output = self.fc_out(dec_out)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "src_vocab_size = 10000  # Size of the source vocabulary\n",
        "tgt_vocab_size = 10000  # Size of the target vocabulary\n",
        "embed_size = 512  # Embedding size\n",
        "num_heads = 8  # Number of attention heads\n",
        "num_enc_layers = 6  # Number of encoder layers\n",
        "num_dec_layers = 6  # Number of decoder layers\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Initialize the transformer\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, embed_size, num_heads, num_enc_layers, num_dec_layers, dropout)\n",
        "\n",
        "# Example input (batch_size=2, sequence_length=5)\n",
        "src = torch.randint(0, src_vocab_size, (2, 5))  # Source sequence\n",
        "tgt = torch.randint(0, tgt_vocab_size, (2, 5))  # Target sequence\n",
        "src_mask = None  # Optional mask for source\n",
        "tgt_mask = None  # Optional mask for target\n",
        "\n",
        "# Forward pass\n",
        "output = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "print(output.shape)  # Output shape will be (batch_size, tgt_sequence_length, tgt_vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# 1. Multi-Head Attention Layer\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_size = embed_size\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_size, \"Embedding size must be divisible by number of heads\"\n",
        "\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask=None):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.num_heads different pieces\n",
        "        values = values.reshape(N, value_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
        "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
        "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim).permute(2, 0, 1, 3)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        energy = torch.einsum(\"nhqd,nhkd->nhqk\", [queries, keys])  # (num_heads, N, query_len, key_len)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=-1)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nhld->nhqd\", [attention, values])  # (num_heads, N, query_len, head_dim)\n",
        "        out = out.permute(1, 2, 0, 3).reshape(N, query_len, self.num_heads * self.head_dim)\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# 2. Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "# 3. Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, embed_size * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size * 4, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "# 4. Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.encoder_attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.norm3 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, embed_size * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size * 4, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, enc_out, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        query = self.dropout(self.norm1(attention + query))\n",
        "        encoder_attention = self.encoder_attention(enc_out, enc_out, query, mask)\n",
        "        x = self.dropout(self.norm2(encoder_attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm3(forward + x))\n",
        "        return out\n",
        "\n",
        "\n",
        "# 5. Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, num_heads, num_enc_layers, num_dec_layers, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.tgt_word_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(embed_size, num_heads, dropout) for _ in range(num_enc_layers)]\n",
        "        )\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(embed_size, num_heads, dropout) for _ in range(num_dec_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        src_embed = self.positional_encoding(self.src_word_embedding(src))\n",
        "        tgt_embed = self.positional_encoding(self.tgt_word_embedding(tgt))\n",
        "\n",
        "        enc_out = src_embed\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_out = layer(enc_out, enc_out, enc_out, src_mask)\n",
        "\n",
        "        dec_out = tgt_embed\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_out = layer(enc_out, enc_out, dec_out, enc_out, tgt_mask)\n",
        "\n",
        "        output = self.fc_out(dec_out)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Example usage of the Transformer for Language Translation:\n",
        "\n",
        "# Assume we have a source and target vocabulary size (for simplicity, set them to 10,000 here).\n",
        "src_vocab_size = 10000  # Example source vocabulary size (e.g., English)\n",
        "tgt_vocab_size = 10000  # Example target vocabulary size (e.g., French)\n",
        "embed_size = 512  # Embedding size\n",
        "num_heads = 8  # Number of attention heads\n",
        "num_enc_layers = 6  # Number of encoder layers\n",
        "num_dec_layers = 6  # Number of decoder layers\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, embed_size, num_heads, num_enc_layers, num_dec_layers, dropout)\n",
        "\n",
        "# Example input data\n",
        "src = torch.randint(0, src_vocab_size, (2, 10))  # Example source sequence (batch_size=2, seq_length=10)\n",
        "tgt = torch.randint(0, tgt_vocab_size, (2, 8))  # Example target sequence (batch_size=2, seq_length=8)\n",
        "\n",
        "# Example masks (can be used for padding or causal masking)\n",
        "src_mask = None  # Assuming no padding mask for simplicity\n",
        "tgt_mask = None  # Assuming no padding mask for simplicity\n",
        "\n",
        "# Forward pass (translation)\n",
        "output = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "print(output.shape)  # Output shape: (batch_size, tgt_seq_len, tgt_vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSXS1RxE0rsE",
        "outputId": "449d4036-b6a7-442c-a0c3-38a7e7fa4636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i5U0npbO0s_q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}