{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UohntFOt0os",
        "outputId": "c888d788-82e6-40e2-9db0-c50464f878eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdJ8RdiCs5uN",
        "outputId": "95ccccbf-8385-42f1-9356-ee6ccd8c1f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ],
      "source": [
        "# import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a sample text\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract lemmatized tokens\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "# Join the lemmatized tokens into a sentence\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsCVWLxxuDpR",
        "outputId": "b1eb379b-12d1-4afb-833a-8c02b6fe7b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown foxes are jumping over the lazy dogs.\n",
            "Lemmatized Text: the quick brown fox be jump over the lazy dog .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Cleaning"
      ],
      "metadata": {
        "id": "hTPLNLq-u3wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "!pip install contractions\n",
        "import contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMa1BzAoubXB",
        "outputId": "31ab1641-a926-419a-e2ee-8c9e93afa2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample, an example file!!! for text Cleaning. There are 20 words in it.\"\n",
        "ans = text.lower()\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gugKODs6u2Xh",
        "outputId": "5825cc8d-2933-451f-ce20-561df553a617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a sample, an example file!!! for text cleaning. there are 20 words in it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "ans = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NcRN18BYu9WJ",
        "outputId": "ee5d878b-9d05-4fe4-b647-2517b836f8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a sample an example file for text Cleaning There are 20 words in it'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans = ''.join([i for i in text if not i.isdigit()])\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zYPCz4csvQlR",
        "outputId": "35c5f35e-bdc8-49f1-85b6-4d36f2e19c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a sample, an example file!!! for text Cleaning. There are  words in it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans = \" \".join(text.split())\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5XazRzNMvuz5",
        "outputId": "8df11a2d-d298-4f55-fc59-b921b7f3d1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a sample, an example file!!! for text Cleaning. There are 20 words in it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"She'd like to know how I'd do that!\"\n",
        "contractions.fix(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P5cmkeCuvyLB",
        "outputId": "1b4b4a10-1702-4a19-d87a-ea787a3f6a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She would like to know how I would do that!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stop Words"
      ],
      "metadata": {
        "id": "kM9yQUY8wi_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyR8PdH-wFmx",
        "outputId": "d64ac39a-69a9-4998-ee20-b878190f70de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence,\n",
        "\t\t\t\tshowing off the stop words filtration.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "# converts the words in word_tokens to lower case and then checks whether\n",
        "#they are present in stop_words or not\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "#with no lower case conversion\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "\tif w not in stop_words:\n",
        "\t\tfiltered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEF7trh9wnLp",
        "outputId": "0e238708-74eb-414a-fb0f-169788e0aefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"There is a pen on the table\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "# Join the filtered words to form a clean text\n",
        "clean_text = ' '.join(filtered_words)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Text after Stopword Removal:\", clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JivmED5Uwwna",
        "outputId": "b97f75bc-bd09-4047-fd35-12edb4156b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: There is a pen on the table\n",
            "Text after Stopword Removal: pen table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "34Oxac_FxviT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset (you can save this as 'data.csv' to test)\n",
        "data = {\n",
        "    'Name': ['Apple', 'Banana', 'Cherry', 'Watermelon', 'Grape', 'Orange', 'Kiwi'],\n",
        "    'Color': ['Red', 'Yellow', 'Red', 'Green', 'Purple', 'Orange', 'Brown'],\n",
        "    'Size': ['Small', 'Medium', 'Small', 'Large', 'Small', 'Medium', 'Medium']\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display unique values of 'Color' column\n",
        "print(\"Unique colors:\", df['Color'].unique())\n",
        "\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Initialize label encoder\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in 'Color' column\n",
        "df['Color'] = label_encoder.fit_transform(df['Color'])\n",
        "\n",
        "# Encode labels in 'Size' column\n",
        "df['Size'] = label_encoder.fit_transform(df['Size'])\n",
        "\n",
        "# Display unique values after encoding\n",
        "print(\"Encoded Color values:\", df['Color'].unique())\n",
        "print(\"Encoded Size values:\", df['Size'].unique())\n",
        "\n",
        "# Show the updated DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oaf2LCnhw_Xh",
        "outputId": "a8d1a5be-8bbe-4587-96e0-9d5258ee598f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique colors: ['Red' 'Yellow' 'Green' 'Purple' 'Orange' 'Brown']\n",
            "Encoded Color values: [4 5 1 3 2 0]\n",
            "Encoded Size values: [2 1 0]\n",
            "         Name  Color  Size\n",
            "0       Apple      4     2\n",
            "1      Banana      5     1\n",
            "2      Cherry      4     2\n",
            "3  Watermelon      1     0\n",
            "4       Grape      3     2\n",
            "5      Orange      2     1\n",
            "6        Kiwi      0     1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYAaguGGxxyS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}